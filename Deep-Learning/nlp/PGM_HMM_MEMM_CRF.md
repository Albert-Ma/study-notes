# PGM HMM MEMM CRF

@author mxy

@date 2018/9/28

为了对CRF有一个全面的了解，以一种追根溯源的方式，从概率无向图（PGM），到隐马尔科夫（HMM），到最大熵模型（MEMM），再到条件随机场（CRF）做
一个粗略的介绍。

## Why PGM

从传统机器学习是否对观察到的样本进行概率建模的角度来看，可分为非概率模型和概率模型。
### 概率模型vs非概率模型

非概率模型指的是直接学习输入空间到输出空间的映射，学习的过程
不涉及概率密度的估计，概率密度的积分等操作，问题的关键在于最优化问题的求解，即如何在输入和输出空间之间找出最优的映射函数出来。通常为了学习h(x),我们
会首先根据先验知识(prior knowledge)选择一个特定的假设空间H(函数空间)，比如所有线性函数构成的假设空间,然后在这个空间找出使得泛化误差最小的假设出来。
$$ h^{\*} = argmin_{h\subset{H}} \varepsilon(h) = argmin_{h\subset{H}} \sum{l(h(x),y)P(x,y)} $$
其中$ l(h(x),y) $ 是我们选取的损失函数，选择不同，则泛化误差结果不同。由于我们并不知道$ P(x,y) $ ,所以即使选好了损失函数，也无法计算出假设的
泛化误差，更别提找出argmin来了。于是，转去寻找使得 经验误差最小 的假设,$$ g =  argmin_{h\subset{H}} \frac{1}{m} \sum{l(h(x^{i}),y^{i}}) $$
这种学习策略叫经验误差最小化(ERM),理论依据是大数定律：当训练样例无穷多的时候，假设的经验误差在概率意义下无限接近假设的泛化误差。

而概率模型则是将学习任务归结于计算变量的概率分布。利用已知变量推测未知变量的分布称为“推断”(inference)，其核心是如何基于可观测变量推测出未知变量
的分布。概率模型指出了学习的目的是学出P(x,y)或者P(x|y),对于P(x,y)的估计，一般是根据乘法公式P(x,y) = P(x|y)P(y)将其拆解成P(x|y),P(y)分
别进行估计。无论是对P(x|y),P(y)还是P(y|x)的估计，都是会先假设分布的形式，例如逻辑斯特回归就假设了Y|X服从伯努利分布。分布形式固定以后，剩下
的就是分布参数的估计问题。常用的估计有极大似然估计(MLE)和极大后验概率估计(MAP)等。

### 问题
具体来说，假定模型所关心的变量集合为Y（Y一般指机器学习模型所需数据的label），可观测变量集合为O（O一般指模型的输入X），其他变量为R（R一般指整个数据集中
，模型中的隐变量，如拿球试验中的袋子），生成式模型考虑联合分布P(Y,R,O),判别式考虑条件分布P(Y,R|O).推断就是要由P(Y,R,O)或P(Y,R|O)得到条件概率分布
P(Y|O). 直接利用概率求和规则消去R显然不可行，因为即便每个变量是二取值，复杂度已至少是$ O(2^{|Y|+|R|}) $, 另一方面，属性之间往往存在复杂的联系
，上一个变量的属性对当前变量的属性值有一定的影响，例如词性标注中的词性；最后如果存在不可观测变量，如何通过已有证据计算它们的条件概率分布也是困难的。
所以为了研究高效的推断和学习算法，需要有一套能简洁紧凑表达变量间关系的工具。

（CMU）概率图模型的三个优势：

1.结合领域知识和因果（逻辑）结构。

2.数据融合。

3.贝叶斯哲学，从数据中学习知识


## PGM


![pgm](images/pgm.jpg)

概率图模型是一类用图来表达变量间相关关系的概率模型，G=(V,E),V表示节点，即随机变量，E表示边，即变量间的概率相关关系。根据边的性质不同，可分为
有向无环图和无向图模型。有向图中最经典的是隐马尔科夫模型。

### 有向图模型
对于有向图模型，联合概率可表示为：$$ P(x_{1},...,x_{n}) = \prod_{i=0} {P(x_{i}|\pi{x_{i}})} $$

举例：![pgm_1](images/pgm_1.png) 

联合概率表示为：
$$ P(x_{1},...,x_{n}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{2})P(x_{4}|x_{2})P(x_{5}|x_{3},x_{4}) $$

### 无向图模型
无向图模型也称为马尔科夫随机场。不同于有向图模型，无向图模型的无向性很难确保每个节点在给定它的邻节点的条件下的条件概率和以图中其他节点为条件的
条件概率一致。由于这个原因，无向图模型的联合概率并不是用条件概率参数化表示的，而是定义为由一组条件独立的局部函数的乘积形式，此函数也称为**势函数**。
亦称“因子”(factor)，这是定义在变量子集上的非负实函数，主要用于定义概率分布函数。常用方法为**因子分解**(Factorization),因子分解就是说将
无向图所描述的联合概率分布表达为若干个子联合概率的乘积，从而便于模型的学习和计算。

![pgm_2](images/pgm_2.png)

上图显示出一个简单的马尔科夫随机场，对于图中节点的一个子集，若其中任意两节点间都有边连接，则称该节点子集为一个“团”(clique)，若在团中加入另外任何一个
几点都不在形成团，则称该团为极大团(maximal clique).如上图中{x1,x3},{x1,x4},{x3,x4},{x2,x3},{x2,x4},{x1,x3,x4},{x2,x3,x4}都是团，其中
{x1,x3,x4},{x2,x3,x4}是极大团，显然每个节点都至少出现在一个极大团中，每个团中都有边相连。

在马尔科夫随机场中，多个变量之间的联合概率分布能基于团分解为多个因子的乘积，每个因子仅与一个团有关。具体来说，对于n个变量，$ x=(x_{1},...,x_{n}) $,
所有团构成的集合为C，与极大团$ Q \in{C} $ 对应的变量集合记为$ x_{Q} $ ,则联合概率P(x)定义为：
$$ P(x)=\frac{1}{Z} \prod_{Q\in{C}} \psi_{Q}(x_{Q}) $$ 

其中$ \psi_{Q} $为与团Q对应的势函数，用于对团Q中的变量关系进行建模，$Z=\sum_{x}\prod_{Q\in{C}}\psi_{Q}(x_{Q})$ 为规范化因子，以确保P(x)
被正确定义的概率。所以像上面的图的中，$ x=(x_{1},...,x_{4}) $,联合概率分布P(x)定义为：
$$ P(x)=\frac{1}{Z}\psi_{1}(x_{1},x_{3},x_{4})\psi_{2}(x_{2},x_{3},x_{4}) $$ 

由于要求势函数严格为正，通常将其定义为指数函数：
$$ \psi_{C}(x)=exp(-E[x]) $$

上面的因子分解过程就是Hammersley-Clifford 定理。

在马尔科夫随机场中如何得到“条件独立性”呢，或者说如何保证概率图是否为概率无向图呢。

如果一个概率图的联合概率分布满足成对、局部或者全局马尔科夫性，就称此联合概率分布为概率无向图模型或马尔科夫随机场。

1.成对马尔科夫性(pairwise Markov property)

设无向图 G 中的任意两个没有边连接的节点 u 、v ，其他所有节点为 O ，成对马尔可夫性指：给定 $ Y_{o} $ 的条件下，$ Y_{u} $ 和 $ Y_{v} $ 条件独立.

![pgm3](images/pgm_3.png)

2.局部马尔科夫性(local Markov property)

设无向图 G 的任一节点 v ，W 是与 v 有边相连的所有节点，O 是 v 、W 外的其他所有节点，局部马尔可夫性指：给定 $ Y_{w} $ 的条件下，$ Y_{v} $ 和 $ Y_{o} $ 条件独立.

![pgm4](images/pgm4.png)

3.全局马尔科夫性(global Markov property)

设节点集合 A 、B 是在无向图 G 中被节点集合 C 分开的任意节点集合，全局马尔可夫性指：给定 $ Y_{C} $ 的条件下，$ Y_{A} $ 和 $ Y_{B} $ 条件独立.

![pgm5](images/pgm5.png)

## HMM

在有向图模型中最经典的应该属于HMM了，不懂的先看这个：[入门级](https://www.zhihu.com/question/20962240)

隐马尔可夫模型（Hidden Markov model，HMM）属于生成式模型，被广泛用于序列标注问题，在语音语言领域里比较出名的应用包括语音识别、中文分词（CWS）、词性标注（POS）等。
隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。
隐藏的马尔可夫链随机生成的状态序列，称为状态序列（state sequence),每一个称为状态变量${x_{1},x_{2},...,x_{n}} $，通常假定状态变量是隐藏的、不可被观测的，因此状态变量
也称为“隐变量”;每个状态变量生成一个观测变量${o_{1},o_{2},...,o_{n}}$，而由此产生的观测的随机序列，称为观测序列（observation sequence)。观测变量的取值仅依赖于状态变量，
即$x_{i}$由$o_{i}$确定，与其他状态变量和观测变量无关；同时，i时刻的状态$o_{i}$仅依赖前一时刻的状态，与其他状态无关，这就是所谓的“马尔可夫链”：
系统下一时刻的状态仅有当前状态决定，不依赖于以往任何状态。基于这种依赖关系，所有变量的联合概率分布为：

![hmm1](images/hmm.png)

上图中的y就是观测变量o。除了观测变量X和隐变量O，HMM 还需要确定状态转移概率A、输出观测概率B、初始状态概率$\pi$。HMM模型需要学习这五组参数(X,O,A,B,$\pi$).

举例来说：应用到词性标注中，x代表词语，是可以观察到的。o代表我们要预测的词性（一个词可能对应多个词性）是隐含状态。应用到分词中，x代表词语，
是可以观察的。o代表我们的标签（B,E这些标签，代表一个词语的开始，或者中间等等）。应用到命名实体识别中，x代表词语，是可以观察的。o代表我们的标
签（标签代表着地点词，时间词这些）.其他的A,B,$\pi$，这些概率分布信息（上帝信息）是需要学习过程中确定的。

HMM的三个基本问题：概率计算、解码和学习问题：

1. 概率计算问题。给定HMM模型λ（A,B,$\pi$）和观测序列o,如何计算给定模型下观测序列o出现的概率 P(O|λ)？前向、后向算法
2. 解码问题。给定一个HMM λ 和一个观测序列 O ，如何最大化 P(Q|O)，求出“最有可能”的状态序列 Q ？维特比算法
3. 学习问题（learning）。给定若干观测序列 O（对应的状态序列 Q 可能已知也可能未知），如何训练出模型参数 λ ，使得 P(O|λ) 最大化？——EM算法

待补充。。。

NLP中的序列标注问题对应于解码问题，首先学习完一个HMM模型之后，通过观测序列找出隐含状态序列，使得该隐含状态序列能以最大概率产生观测序列。

## MEMM
由于最大熵模型中的特征函数和CRF中的类似，所以先介绍最大熵模型作为过渡。

还是先堆点基础知识吧。

![memm1](images/memm1.png)

**熵**是随机变量不确定性的度量，不确定性越大，熵值就越大；若随机变量退化成定值，熵为0。均匀分布是“最不确定”的分布。假设离散随机变量X的概率分
布为P(x)，则其熵为：$$ H(P) = - \sum_{x}P(x)logP(x) $$ $$ 0 \le{H(P)} \le{log|X|}$$
**联合熵**是X,Y同时发生的不确定性。$$ H(X,Y) = -\sum_{x,Y}P(x,y)logP(x,y) $$
**条件熵**是说已知一个变量发生的条件下，另一个变量发生新增的不确定性，即X确定时，Y的不确定性度量。$$ H(Y|X) = H(X,Y) - H(X) $$
**互信息**是说已知X发生的条件下，Y不确定性减少的程度，决策树中也叫信息增益。$$ I(X,Y) = H(Y) - H(Y|X) $$

最大熵模型是判别式模型，直接为条件概率P(Y|X)建模，X为特征,Y为输出。他是由最大熵原理推导实现的，最大熵原理是概率模型学习的一个准则。
最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理
也可以表述为在满足约束条件的模型集合中选择熵最大的模型。

直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即**约束条件**，在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的
最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值目标。

### 约束条件
最大熵模型要满足约束条件，这也限制了模型的取值范围。举例说明约束条件：比如在NER问题上，当前的$i$的标注不仅跟当前观测到的单词有关，跟前后标注，字母大小写，词性等等有关
。这些每一个都是一个约束条件。为了描述这些约束条件，提出一个新的名词**特征函数**$f(x,y)$用来描述输入x和输出y之间的某一个事实。
比如特征函数可以这么定义：
$$ f_{a}(o,i) = 1 ,  满足特定条件. \\
= 0, other $$
它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0.

最大熵模型由于要对条件概率$P(Y|X)$ 建模求熵，然后使之最大化，在满足约束条件的情况下。从熵的计算公式$$H(Y|X)=H(X,Y) - H(X) = -\sum_{x,y}P(x,y)logP(y|x)$$
来看是需要知道$P(y|x)$，由贝叶斯公式$p(x,y) =p(x)p(y|x)$，但是我们是不知道真实的$p(x,y), p(x)$的，我们只能从训练数据中模拟真实分布，得到一个被称为“经验分布”的近似值:
$$\bar(P)(X,Y)=\frac{count(x,y)}{N},\bar(P)(x)=\frac{count(x)}{N}$$

其中count为样本出现的次数，N为样本总数。 既然是近似值，是经验分布，那么必须对“近似”有个约束，使得特征函数$f(x,y)$关于经验分布$\bar(P)(X,Y)$
的期望值和特征函数$f(x,y)$模型关于$P(Y|X)$和经验分布$\bar(P)(x)$的期望值相等。即
$$ E_{\bar(p)}(f)=\sum_{x,y}\bar(P)(x,y)f(x,y)=E_{p}(f)=\sum_{x,y}\bar(P)(x)P(y|x)f(x,y)$$

此等式是最大熵模型学习的默认约束。

### 最大熵模型
假设满足所有约束条件的模型集合为：
$$C = {P\in(T)|E_{p}(f_{i}) = E_{\bar(p)(f_{i})},i=1,2,3...,n}$$
定义在条件概率分布上P(Y|X)的条件熵为$$H(P)=-\sum_{x,y}\bar(P)(x)P(y|x)logP(y|x)$$
则模型集合C中条件熵H(P)最大的模型称为最大熵模型。

### 模型求解
待补充。。。

1. 将目标函数转变为对偶函数，是凸函数，通过梯度下降等方法进行模型的学习
2. 极大似然估计方法


最大熵模型更一般的形式：
$$P_{w}(y|x) = \frac{1}{Z_{w}}(x)exp(\sum_{i=1}^{n}w_{i}f_{i}(x,y))$$
其中
$$Z_{w}(x)=\sum_{y}exp(\sum_{i=1}^{n}w_{i}f_{i}(x,y))$$
这里，$x\in(R^{n})$,$y\in({1,2,...,k})$为输出，$w\in(R^n)$为权重向量，$f_{i}$为任意实值特征函数。

最大熵模型与logistic模型有类似形式，他们又称为对数线性模型（log linear model）。

![memm2](images/memm2.png)
### 标注偏置
labeling bias问题是说在viterbi算法解码的时候出现的，MEMM倾向于选择拥有更少转移的状态。

![memm3](images/memm3.png)

待补充。。。

## CRF
Crf是概率图模型中的无向图类型中最经典的一个了。 条件随机场（Conditional random field，CRF）是条件概率分布模型 P(Y|X) ，表示的是给定一组
输入随机变量 X ，即观测变量的条件下另一组输出随机变量 Y ,也就是隐变量的马尔可夫随机场，也就是说 CRF 的特点是假设**输出随机变量Y**构成马尔可夫随机场，
这里很重要！！！！！！！！

条件随机场可以看做是最大熵马尔科夫模型在标注问题的推广，这里主要介绍线性链式条件随机场。

![crf5](images/crf5.png)

![crf1](images/crf1.png)

从上图可以看出，crf其实是有条件的HMM，HMM是生成式模型，对联合概率建模，而Crf是判别式模型，对条件概率建模。
广义的crf定义是：若随机变量Y构成一个由无向图$G = (V,E)$表示的马尔科夫场，即
$$ P(Y_{v}|X,Y_{w},w\ne(v))= P(Y_{v}|X,Y_{w},w\sim(v)) $$
对任意节点$v$成立,则称条件概率分布$P(Y|X)$为条件随机场。式中$w\sim(v)$是说图中与节点v有边连接的所有节点w，$w\ne(v)$表示节点v以外的所有节点。
这也看不出条件随机场的特性来啊，那把等式两边的相同条件X遮住，那么式子可以用下图示意：

![crf4](images/crf4.png)

可以看出上图就是局部马尔科夫性，在$Y_{v}$条件下，$Y_{w}$和$Y_{o}$条件独立。明白

在定义中并没有要求 X 和 Y 具有相同的结构，而在现实中，一般假设 X 和 Y 有相同的图结构。对于线性链条件随机场来说，图 G 的每条边都存在于状态序
列 Y 的相邻两个节点，最大团 C 是相邻两个节点的集合，X 和 Y 有相同的图结构意味着每个 $X_{i}$ 都与 $Y_{i}$ 一一对应。

设两组随机变量 $X=(X_{1},...,X_{n}),Y=(Y_{1},...,Y_{n})$ 均为线性链表示的随机变量序列，若在给定随机变量序列X的条件下，随机变量序列Y的
条件概率分布$P(Y|X)$构成条件随机场，即满足马尔科夫性，那么线性链条件随机场的定义为：
$$P(Y_{i}|X,Y_{1},...,Y_{i-1},Y_{i+1},..,Y_{n})=P(Y_{i}|X,Y_{i-1},Y_{i+1}),i=1,...,n(在i=1和n时只考虑单边)$$

在标注问题中，X表示输入,即观测序列，Y表示对应的输出标记序列或状态序列。
### 线性链条随机场的参数化形式（特征函数）
此前我们知道，马尔可夫随机场可以利用最大团的函数来做因子分解。如下通用形式：
$$ P(x)=\frac{1}{Z} \prod_{Q\in{C}} \psi_{Q}(x_{Q}) $$ 

引入指数函数的势函数之后，crf建模公式如下：

![crf2](images/crf2.png)

可以看出crf公式和softmax，memm公式很像，它们都属于对数线性模型（log linear model），线性链CRF可以用来解决序列标注问题，Softmax回归、最大
熵模型基本都是用来解决分类问题。

crf的线性链条件随机场的参数化形式如下图，可由马尔科夫随机场的因子分解递推得到：

![crf3](images/crf3.png)

关于如上的参数化表示形式的推导可理解为，在由因子分解公式，到势函数变为指数形式，再到将一个图的边和节点分开，也就是说将原本的特征函数$f$,拆分为
表示边的$f_{k}(y_{i},y_{i-1},x)$和表示节点的$g_{l}(y_{i},x)$.

式中$f_{k}$被定义为转移特征函数，依赖于当前和前一个位置；$g_{l}$定义为状态特征函数，依赖于当前位置。$\lambda,\mu$是权值，表示特征函数的重要性。
通常特征函数的取值0或1；当满足特定条件时取值为1，否则为0，$Z$是规范化因子.

在序列标注问题中，x是观测序列，也就是token单词，y是词性，特征函数代表当前词性的特征，有了特征打分之后，将所得的分数log线性表示求和后归一化（softmax）
，即可得到概率值。所以在模型训练之前，我们首先要定义好特征函数，比如单词大小写，前一个单词词性等等，然后训练模型，确定参数，最后进行预测，也就是
序列标注，采用viterbi算法求出最佳路径。

## HMM vs. MEMM vs. CRF
将三者放在一块做一个总结：

1. HMM -> MEMM： 

HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅
和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM
引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。

2. MEMM -> CRF:

CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了
全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。

HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。



## Reference

1. 机器学习第十四章。周志华著

2. [Probabilistic Graphical Models](http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html)

3. [如何用简单易懂的例子解释crf](https://www.zhihu.com/question/35866596)

4. [NLP图模型](https://www.cnblogs.com/Determined22/p/6915730.html)

5. [一步一步理解最大熵](https://www.cnblogs.com/wxquare/p/5858008.html)

6. [crf-tutorial](https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf)

7. [监督学习的分类](https://zhuanlan.zhihu.com/p/26012348)

8. [隐马尔科夫](https://zhuanlan.zhihu.com/p/26811689)

9. 统计学习方法第六章。李航著
